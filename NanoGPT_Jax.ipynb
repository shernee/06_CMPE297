{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shernee/06_CMPE297/blob/main/NanoGPT_Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax\n",
        "!pip install optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ciz8pdAg8eo",
        "outputId": "ef89609a-a0af-4800-df5c-eb893af3e1e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from flax) (1.23.5)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.10/dist-packages (from flax) (0.3.25)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from flax) (3.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.7)\n",
            "Requirement already satisfied: orbax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.2)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from flax) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->flax) (1.11.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.6)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.3.25)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.10/dist-packages (from orbax->flax) (1.5.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from orbax->flax) (6.1.0)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.10/dist-packages (from orbax->flax) (1.5.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->flax) (1.16.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.6)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax) (0.3.25)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax) (0.3.25)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.23.5)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (4.5.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (1.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.nn import softmax\n",
        "from jax.random import categorical\n",
        "import jax\n",
        "from jax import grad, jit\n",
        "from flax import linen as nn\n",
        "from flax.core import freeze, unfreeze\n",
        "from jax.nn import softmax\n",
        "import optax\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "x9t529IT580a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZXeAQDXwFAQ",
        "outputId": "d0d1bc9b-cf90-4a6a-e141-ac62c070336d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 2500\n",
        "eval_interval = 200\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 100\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 8"
      ],
      "metadata": {
        "id": "DeADGpnw6C9j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key = jax.random.PRNGKey(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsTFJWGf6nyB",
        "outputId": "92e78598-6f14-48b2-fee8-e8bcdedfaafe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"/content/drive/MyDrive/297_data/Sorcerer's_stone.txt\""
      ],
      "metadata": {
        "id": "0hJ_s3WvwJ0l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbpfhHeoxVSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 0\n",
        "    self.train_data = jnp.array([], dtype=jnp.int32)\n",
        "    self.val_data = jnp.array([], dtype=jnp.int32)\n",
        "\n",
        "  def read_dataset(self):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        self.data = f.read()\n",
        "\n",
        "  def prepare_dataset(self):\n",
        "    self.read_dataset()\n",
        "\n",
        "    chars = sorted(list(set(self.data)))\n",
        "    self.vocab_size = len(chars)\n",
        "    char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "    int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "    self.encode = lambda s: [char_to_int[c] for c in s]\n",
        "    self.decode = lambda l: ''.join([int_to_char[i] for i in l])\n",
        "\n",
        "  def data_split(self):\n",
        "    self.prepare_dataset()\n",
        "\n",
        "    data_tensor = jnp.array(self.encode(self.data), dtype=jnp.int32)\n",
        "    n = int(0.8 * len(data_tensor))\n",
        "    self.train_data = data_tensor[:n]\n",
        "    self.val_data = data_tensor[n:]\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    self.data_split()\n",
        "\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = random.randint(rng_key, (batch_size,), 0, len(data) - block_size)\n",
        "    x = jnp.stack([data[i:i+block_size] for i in ix])\n",
        "    y = jnp.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rrd7yijpOcIZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def estimate_loss(self):\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = []\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = datasetObj.get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses.append(loss)\n",
        "        out[split] = jnp.mean(jnp.array(losses))\n",
        "    return out\n",
        "\n",
        "lossObj = Loss()"
      ],
      "metadata": {
        "id": "kYGFSWWJRqfb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    head_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.key = nn.Dense(self.head_size, use_bias=False, kernel_init=nn.initializers.xavier_uniform())\n",
        "        self.query = nn.Dense(self.head_size, use_bias=False, kernel_init=nn.initializers.xavier_uniform())\n",
        "        self.value = nn.Dense(self.head_size, use_bias=False, kernel_init=nn.initializers.xavier_uniform())\n",
        "        self.tril = jnp.tril(jnp.ones((block_size, block_size)))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        w = jnp.matmul(q, jnp.transpose(k, (0, 2, 1))) * C ** -0.5\n",
        "        w = jnp.where(self.tril[:T, :T] == 0, float('-inf'), w)\n",
        "        w = nn.softmax(w, axis=-1)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = jnp.matmul(w, v)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "bTVi1D-uSKQT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    n_head: int\n",
        "    head_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.heads = [AttentionHead(self.head_size) for _ in range(self.n_head)]\n",
        "        self.proj = nn.Dense(n_embd)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n",
        "        return self.proj(out)"
      ],
      "metadata": {
        "id": "IPrnCUFiUCB5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def setup(self):\n",
        "    self.net = nn.Sequential([\n",
        "        nn.Dense(4 * n_embd),\n",
        "        nn.relu,\n",
        "        nn.Dense(n_embd)\n",
        "    ])\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "PMABKHfSUJX6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def setup(self):\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head=n_head, head_size=head_size)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(epsilon=1e-6)\n",
        "        self.ln2 = nn.LayerNorm(epsilon=1e-6)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "aV8Trt7HUVkE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, idx):\n",
        "        return nn.Dense(n_embd, use_bias=False)(jax.nn.one_hot(idx, datasetObj.vocab_size))\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "    def setup(self):\n",
        "        self.T = None  # Initialize T as None\n",
        "\n",
        "    def set_T(self, T):\n",
        "        self.T = T  # Set T dynamically\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx):\n",
        "        assert self.T is not None, \"T must be set using set_T() before calling PositionEmbedding\"\n",
        "        return nn.Dense(n_embd, use_bias=False)(jax.nn.one_hot(jnp.arange(self.T), block_size))\n",
        "\n",
        "class NanoGPT(nn.Module):\n",
        "    def setup(self):\n",
        "        self.blocks = [TransformerBlock() for _ in range(n_layer)]\n",
        "        self.ln_f = nn.LayerNorm(epsilon=1e-6)\n",
        "        self.lm_head = nn.Dense(datasetObj.vocab_size, kernel_init=nn.initializers.xavier_uniform())\n",
        "\n",
        "        # Initialize submodules within setup\n",
        "        self.token_embedding = TokenEmbedding()\n",
        "        self.position_embedding = PositionEmbedding()\n",
        "\n",
        "    def __call__(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        self.position_embedding.set_T(T)  # Set T dynamically\n",
        "        pos_emb = self.position_embedding(jnp.arange(T))\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = jnp.mean(jax.nn.softmax_cross_entropy(logits, jax.nn.one_hot(targets, datasetObj.vocab_size)))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, key):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = nn.softmax(logits)\n",
        "            idx_next = random.categorical(key, logits)\n",
        "            idx = jnp.concatenate([idx, idx_next[:, None]], axis=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "6wlMV8JYUjiL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateNext():\n",
        "  key = random.PRNGKey(0)\n",
        "  context = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "  generated_seq = model.generate(context, max_new_tokens=2000, key=key)\n",
        "  print(datasetObj.decode(generated_seq[0].tolist()))"
      ],
      "metadata": {
        "id": "dyCbV9XLWH5q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  datasetObj = Dataset()\n",
        "  datasetObj.read_dataset()\n",
        "  datasetObj.prepare_dataset()\n",
        "\n",
        "  model = NanoGPT()\n",
        "  params = model.init(rng_key, jnp.zeros((1, block_size), dtype=jnp.int32))\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=learning_rate)\n",
        "  state = optimizer.init(params)\n",
        "\n",
        "  @jit\n",
        "  def update(params, xb, yb, state):\n",
        "      logits, loss = model.apply({'params': params}, xb, yb)\n",
        "      grads = jax.grad(loss)(params)\n",
        "      updates, new_state = optimizer.update(grads, state)\n",
        "      new_params = optax.apply_updates(params, updates)\n",
        "      return new_params, loss, new_state\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "      if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "          losses = lossObj.estimate_loss()\n",
        "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "      xb, yb = datasetObj.get_batch('train')\n",
        "      params, loss, state = update(params, xb, yb, state)"
      ],
      "metadata": {
        "id": "bvufiICcZj9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generateNext()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de42eadb-5d8e-41dd-cfbd-25372f2fdb8f",
        "id": "FVCVRX_tZHkB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tsitment dye, and a eap secred the\n",
            "gian; I would yet you.\" Hagrid But's gave\n",
            "to rem the fircelie to\n",
            "peope sor house\n",
            "pit lone wat on the grown.\n",
            "\n",
            "Hagwarts a wonder fatter, and dadn't hand no\n",
            "foufffer ap read-of Hathil\n",
            "rough bying up SlitKen?\" Maltch their slieet look om row -- he Gather Hermioniton\n",
            "would swing he tock more the ark.\n",
            "\n",
            "\"I happearen over, boy magming sixty.\n",
            "Everywhere lot up. Eve a whating gad, with\n",
            "their happened and of there, Harry now, looked at the\n",
            "madking people spinare you really -- the Weasle's\n",
            "and in realied there was almodicking.\n",
            "\n",
            "\"A keepings called. \n",
            "A did. Wand talking and fanglimers, choocked unnider the\n",
            "pain witches off the were outs) of father's into hot big bed\n",
            "plick on when he gripped again.\n",
            "\n",
            "\"What's five of it.\"\n",
            "\n",
            "Hagrid minute to knay, as Quidditch my and\n",
            "the\n",
            "strick that man that ganting to didn't be a lot asside off it, eever dragons and ortoad no fuart for put --\n",
            "the take would him was a dawn was, you the Malfoy.\n",
            "\n",
            "\"Lee we with around if it!\" said Finning the storcittomage fromilates.\n",
            "\n",
            "\"I keape doing the carfled boookst off difficuls.\n",
            "\n",
            "\"Mismy behinds -- who lower some over as daing?\"\n",
            "\n",
            "\"Tell, andy capting\n",
            "Harry's moved more broom.\"\n",
            "\n",
            "Malfoy.\n",
            "\n",
            "\"Yeh, soones pippet - I'MMR!\"\n",
            "\n",
            "Haggid, who go a nuck afre been only the nigh, but you heare was in unut the\n",
            "Boun you'd Cromber Potter.\"\n",
            "\n",
            "Hagrid had madden twig about the\n",
            "ap from,\n",
            "under compled out moment myster,\" Hagrid ForJs,\n",
            "groted cliar swing falling\n",
            "and anottly fard witthat looked right alf them\n",
            "he'd taken at noigh,\n",
            "and\n",
            "Harry Hermione had\n",
            "stricked outo it the couldn't on the book voice with Hagrid was who wore low where as all almost.\"\n",
            "\n",
            "The stilt gaspered off them.\n",
            "\n",
            "\"Hig's almost stop car Hagrid. \"Nothing if he looked in what been een all witchen softed the teardzase, if the were starts? Peole's hoke all never and\n",
            "no poise, before yet mirr.\n",
            "\n",
            "\"I'll really one want down at all her?\"\n",
            "\n",
            "\"So lother ordder in something,\" said Hagrid, starping\n",
            "wants anything\n",
            "the\n",
            "witchen forts blinding at the lant.\n",
            "\n",
            "\"I make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7LUFqXkajFLx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}