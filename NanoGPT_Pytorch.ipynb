{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shernee/06_CMPE297/blob/main/NanoGPT_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "x9t529IT580a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1p4m55mtQca",
        "outputId": "4f20ac42-184e-4552-d807-b5e59ef22b64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(101)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WszbuVk6Hg-",
        "outputId": "fa46e7d5-5169-45ca-90ec-5caf206f744e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f30dcb3bb70>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 500\n",
        "n_embd = 64\n",
        "n_head = 8\n",
        "n_layer = 8"
      ],
      "metadata": {
        "id": "DeADGpnw6C9j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"/content/drive/MyDrive/297_data/Sorcerer's_stone.txt\""
      ],
      "metadata": {
        "id": "k5imSmEatdK1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 0\n",
        "    self.train_data = torch.tensor([])\n",
        "    self.val_data = torch.tensor([])\n",
        "\n",
        "  def read_dataset(self):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        self.data = f.read()\n",
        "\n",
        "  def prepare_dataset(self):\n",
        "    chars = sorted(list(set(self.data)))\n",
        "    self.vocab_size = len(chars)\n",
        "    char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "    int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "    self.encode = lambda s: [char_to_int[c] for c in s]\n",
        "    self.decode = lambda l: ''.join([int_to_char[i] for i in l])\n",
        "\n",
        "  def data_split(self):\n",
        "    data_tensor = torch.tensor(self.encode(self.data), dtype=torch.long)\n",
        "    n = int(0.8*len(data_tensor))\n",
        "    self.train_data = data_tensor[:n]\n",
        "    self.val_data = data_tensor[n:]\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rrd7yijpOcIZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetObj = Dataset()\n",
        "datasetObj.read_dataset()\n",
        "datasetObj.prepare_dataset()\n",
        "datasetObj.data_split()"
      ],
      "metadata": {
        "id": "oSlIFHNmuGDV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(self):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = datasetObj.get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "kYGFSWWJRqfb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossObj = Loss()"
      ],
      "metadata": {
        "id": "vuFmZim7um45"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    w = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    w = F.softmax(w, dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = w @ v\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "bTVi1D-uSKQT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "IPrnCUFiUCB5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "PMABKHfSUJX6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedFoward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "aV8Trt7HUVkE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoGPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(datasetObj.vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, datasetObj.vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "6wlMV8JYUjiL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateNext():\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print(datasetObj.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "dyCbV9XLWH5q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TozLG9XW53XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef5e4d4-c293-4914-cf63-3aed8f04b6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.410703 M parameters\n",
            "step 0: train loss 4.5534, val loss 4.5544\n",
            "step 1000: train loss 1.9980, val loss 1.9984\n",
            "step 2000: train loss 1.7309, val loss 1.7467\n",
            "step 3000: train loss 1.6159, val loss 1.6587\n",
            "step 4000: train loss 1.5385, val loss 1.5918\n",
            "step 4999: train loss 1.4940, val loss 1.5752\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = NanoGPT()\n",
        "  m = model.to(device)\n",
        "  print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = lossObj.estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = datasetObj.get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generateNext()"
      ],
      "metadata": {
        "id": "6oEiOyZyWNGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de42eadb-5d8e-41dd-cfbd-25372f2fdb8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tsitment dye, and a eap secred the\n",
            "gian; I would yet you.\" Hagrid But's gave\n",
            "to rem the fircelie to\n",
            "peope sor house\n",
            "pit lone wat on the grown.\n",
            "\n",
            "Hagwarts a wonder fatter, and dadn't hand no\n",
            "foufffer ap read-of Hathil\n",
            "rough bying up SlitKen?\" Maltch their slieet look om row -- he Gather Hermioniton\n",
            "would swing he tock more the ark.\n",
            "\n",
            "\"I happearen over, boy magming sixty.\n",
            "Everywhere lot up. Eve a whating gad, with\n",
            "their happened and of there, Harry now, looked at the\n",
            "madking people spinare you really -- the Weasle's\n",
            "and in realied there was almodicking.\n",
            "\n",
            "\"A keepings called. \n",
            "A did. Wand talking and fanglimers, choocked unnider the\n",
            "pain witches off the were outs) of father's into hot big bed\n",
            "plick on when he gripped again.\n",
            "\n",
            "\"What's five of it.\"\n",
            "\n",
            "Hagrid minute to knay, as Quidditch my and\n",
            "the\n",
            "strick that man that ganting to didn't be a lot asside off it, eever dragons and ortoad no fuart for put --\n",
            "the take would him was a dawn was, you the Malfoy.\n",
            "\n",
            "\"Lee we with around if it!\" said Finning the storcittomage fromilates.\n",
            "\n",
            "\"I keape doing the carfled boookst off difficuls.\n",
            "\n",
            "\"Mismy behinds -- who lower some over as daing?\"\n",
            "\n",
            "\"Tell, andy capting\n",
            "Harry's moved more broom.\"\n",
            "\n",
            "Malfoy.\n",
            "\n",
            "\"Yeh, soones pippet - I'MMR!\"\n",
            "\n",
            "Haggid, who go a nuck afre been only the nigh, but you heare was in unut the\n",
            "Boun you'd Cromber Potter.\"\n",
            "\n",
            "Hagrid had madden twig about the\n",
            "ap from,\n",
            "under compled out moment myster,\" Hagrid ForJs,\n",
            "groted cliar swing falling\n",
            "and anottly fard witthat looked right alf them\n",
            "he'd taken at noigh,\n",
            "and\n",
            "Harry Hermione had\n",
            "stricked outo it the couldn't on the book voice with Hagrid was who wore low where as all almost.\"\n",
            "\n",
            "The stilt gaspered off them.\n",
            "\n",
            "\"Hig's almost stop car Hagrid. \"Nothing if he looked in what been een all witchen softed the teardzase, if the were starts? Peole's hoke all never and\n",
            "no poise, before yet mirr.\n",
            "\n",
            "\"I'll really one want down at all her?\"\n",
            "\n",
            "\"So lother ordder in something,\" said Hagrid, starping\n",
            "wants anything\n",
            "the\n",
            "witchen forts blinding at the lant.\n",
            "\n",
            "\"I make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7vqctsBGZ_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}